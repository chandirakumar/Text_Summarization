{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmLEdqfQO1FIf6Wj4cSX/z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandirakumar/Text_Summarization/blob/main/Auto_Generated_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets contractions -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqJSVqfP0SV7",
        "outputId": "5a7a3d7e-f60e-4a14-f085-d9450f4fde54"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pG95og6Ka_YP",
        "outputId": "c58f7bd3-5b10-48f4-af70-72e4cda606e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# prompt: create full script for abstractive text summarization which will tokenize the data and finetune the bart cnn model and calculate rogue score\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "from datasets import load_dataset, load_from_disk, load_metric, load_metric\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from contractions import contractions_dict\n",
        "import re\n",
        "from matplotlib import pyplot as plt\n",
        "from transformers import BertTokenizer, BertForMaskedLM, BertModel, BartForConditionalGeneration, BartTokenizer\n",
        "from nltk.corpus import brown\n",
        "import string\n",
        "from random import randint\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, AutoConfig, AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the CNN/Daily Mail dataset\n",
        "cnn_dm_dataset = load_dataset(\"cnn_dailymail\",  '3.0.0', split=\"train\")"
      ],
      "metadata": {
        "id": "zDKUx6rr1MrW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Preprocess the data\n",
        "def preprocess_data(dataset):\n",
        "    # Convert the text to lowercase\n",
        "    dataset[\"article\"] = str(dataset[\"article\"]).lower()\n",
        "    dataset[\"highlights\"] = str(dataset[\"highlights\"]).lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    dataset[\"article\"] = str(dataset[\"article\"]).replace(r\"[^a-zA-Z\\s]\", \"\")\n",
        "    dataset[\"highlights\"] = str(dataset[\"highlights\"]).replace(r\"[^a-zA-Z\\s]\", \"\")\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    dataset[\"article\"] = dataset.map(lambda x: \" \".join([word for word in str(x['article']).split() if word not in stop_words]))\n",
        "    dataset[\"highlights\"] = dataset.map(lambda x: \" \".join([word for word in str(x['highlights']).split() if word not in stop_words]))\n",
        "\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = nltk.WordNetLemmatizer()\n",
        "    dataset[\"article\"] = dataset.map(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in str(x[\"article\"]).split()]))\n",
        "    dataset[\"highlights\"] = dataset.map(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in str(x[\"highlights\"]).split()]))\n",
        "\n",
        "    # Truncate the articles and highlights to a maximum length\n",
        "    dataset[\"article\"] = dataset.map(lambda x: x[\"article\"][:1024])\n",
        "    dataset[\"highlights\"] = dataset.map(lambda x: x[\"highlights\"][:128])\n",
        "\n",
        "    return dataset\n",
        "\n",
        "cnn_dm_dataset = preprocess_data(cnn_dm_dataset)"
      ],
      "metadata": {
        "id": "tTZ5c9GpbFmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "# Create a model\n",
        "model_bart_cnn = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\").to(device)\n",
        "\n",
        "# Create a data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model_bart_cnn)\n",
        "\n",
        "# Create the dataloaders\n",
        "train_dataloader = DataLoader(cnn_dm_dataset[\"train\"], batch_size=16, shuffle=True, collate_fn=data_collator)\n",
        "test_dataloader = DataLoader(cnn_dm_dataset[\"test\"], batch_size=16, shuffle=False, collate_fn=data_collator)\n",
        "\n",
        "# Set the optimizer\n",
        "optimizer = torch.optim.AdamW(model_bart_cnn.parameters(), lr=1e-5)\n",
        "\n",
        "# Set the learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=3)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # Train\n",
        "    model_bart_cnn.train()\n",
        "    train_loss = 0\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        batch = batch.to(device)\n",
        "        outputs = model_bart_cnn(**batch)\n",
        "        loss = outputs.loss\n",
        "        train_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model_bart_cnn.eval()\n",
        "    val_loss = 0\n",
        "    for batch in tqdm(test_dataloader):\n",
        "        batch = batch.to(device)\n",
        "        outputs = model_bart_cnn(**batch)\n",
        "        loss = outputs.loss\n",
        "        val_loss += loss.item()\n",
        "\n",
        "    # Logging\n",
        "    print(f\"Epoch {epoch+1}: Train loss = {train_loss/len(train_dataloader):.4f}, Val loss = {val_loss/len(test_dataloader):.4f}\")\n",
        "\n",
        "    # Update the learning rate\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "# Save the model\n",
        "model_bart_cnn.save_pretrained(\"my_bart_cnn_model\")\n"
      ],
      "metadata": {
        "id": "wWmXB8aCbFhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the model\n",
        "model_bart_cnn = AutoModelForSeq2SeqLM.from_pretrained(\"my_bart_cnn\").to(device)\n"
      ],
      "metadata": {
        "id": "q-B3jHgJbFeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate summaries\n",
        "def generate_summary(article_text):\n",
        "    inputs = tokenizer.encode(\"summarize: \" + article_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model_bart_cnn.generate(inputs, max_length=120, min_length=30, length_penalty=2.0, num_beams=6, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "89iMEh4AbFYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "def evaluate_model(model, dataset):\n",
        "    rouge = load_metric(\"rouge\")\n",
        "    for article, summary in zip(dataset[\"article\"], dataset[\"highlights\"]):\n",
        "        generated_summary = generate_summary(article)\n",
        "        scores = rouge.compute(predictions=[generated_summary], references=[summary])\n",
        "        print(f\"Original summary: {summary}\")\n",
        "        print(f\"Generated summary: {generated_summary}\")\n",
        "        print(f\"ROUGE-L: {scores['rouge-l']['l'].item():.4f}\")\n",
        "        print(f\"ROUGE-2: {scores['rouge-2']['f'].item():.4f}\")\n",
        "        print(f\"ROUGE-1: {scores['rouge-1']['f'].item():.4f}\")\n",
        "\n",
        "evaluate_model(model_bart_cnn, cnn_dm_dataset[\"test\"])"
      ],
      "metadata": {
        "id": "qgSXM2UEbFWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: abstractive text summarization with t5 base train and finetune the cnn data for test summarization\n",
        "\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from contractions import contractions_dict\n",
        "import re\n",
        "from matplotlib import pyplot as plt\n",
        "from transformers import BertTokenizer, BertForMaskedLM, BertModel, BartForConditionalGeneration, BartTokenizer\n",
        "from nltk.corpus import brown\n",
        "import string\n",
        "from random import randint\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, AutoConfig, AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the CNN/Daily Mail dataset\n",
        "cnn_dm_dataset = load_dataset(\"cnn_dailymail\",  '3.0.0', split=\"train\")\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_data(dataset):\n",
        "    # Convert the text to lowercase\n",
        "    dataset[\"article\"] = dataset[\"article\"].lower()\n",
        "    dataset[\"highlights\"] = dataset[\"highlights\"].lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    dataset[\"article\"] = str(dataset[\"article\"]).replace(r\"[^a-zA-Z\\s]\", \"\")\n",
        "    dataset[\"highlights\"] = str(dataset[\"highlights\"]).replace(r\"[^a-zA-Z\\s]\", \"\")\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    dataset[\"article\"] = dataset.map(lambda x: \" \".join([word for word in str(x['article']).split() if word not in stop_words]))\n",
        "    dataset[\"highlights\"] = dataset.map(lambda x: \" \".join([word for word in str(x['highlights']).split() if word not in stop_words]))\n",
        "\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = nltk.WordNetLemmatizer()\n",
        "    dataset[\"article\"] = dataset.map(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in str(x[\"article\"]).split()]))\n",
        "    dataset[\"highlights\"] = dataset.map(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in str(x[\"highlights\"]).split()]))\n",
        "\n",
        "    # Truncate the articles and highlights to a maximum length\n",
        "    dataset[\"article\"] = dataset.map(lambda x: x[\"article\"][:1024])\n",
        "    dataset[\"highlights\"] = dataset.map(lambda x: x[\"highlights\"][:128])\n",
        "\n",
        "    return dataset\n",
        "\n",
        "cnn_dm_dataset = preprocess_data(cnn_dm_dataset)\n",
        "\n",
        "# Create a tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "# Create a model\n",
        "model_bart_cnn = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\").to(device)\n",
        "\n",
        "# Create a data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model_bart_cnn)\n",
        "\n",
        "# Create the dataloaders\n",
        "train_dataloader = DataLoader(cnn_dm_dataset[\"train\"], batch_size=16, shuffle=True, collate_fn=data_collator)\n",
        "test_dataloader = DataLoader(cnn_dm_dataset[\"test\"], batch_size=16, shuffle=False, collate_fn=data_collator)\n",
        "\n",
        "# Set the optimizer\n",
        "optimizer = torch.optim.AdamW(model_bart_cnn.parameters(), lr=1e-5)\n",
        "\n",
        "# Set the learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=3)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # Train\n",
        "    model_bart_cnn.train()\n",
        "    train_loss = 0\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        batch = batch.to(device)\n",
        "        outputs = model_bart_cnn(**batch)\n",
        "        loss = outputs.loss\n",
        "        train_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model_bart_cnn.eval()\n",
        "    val_loss = 0\n",
        "    for batch in tqdm(test_dataloader):\n",
        "        batch = batch.to(device)\n",
        "        outputs = model_bart_cnn(**batch)\n",
        "        loss = outputs.loss\n",
        "        val_loss += loss.item()\n",
        "\n",
        "    # Logging\n",
        "    print(f\"Epoch {epoch+1}: Train loss = {train_loss/len(train_dataloader):.4f}, Val loss = {val_loss/len(test_dataloader):.4f}\")\n",
        "\n",
        "    # Update the learning rate\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "# Save the model\n",
        "model_bart_cnn.save_pretrained(\"my_bart_cnn_model\")\n",
        "\n",
        "# Load the model\n",
        "model_bart_cnn = AutoModelForSeq2SeqLM.from_pretrained(\"my_bart_cnn\").to(device)\n",
        "\n",
        "# Generate summaries\n",
        "def generate_summary(article_text):\n",
        "    inputs = tokenizer.encode(\"summarize: \" + article_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model_bart_cnn.generate(inputs, max_length=120, min_length=30, length_penalty=2.0, num_beams=6, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model(model, dataset):\n",
        "    rouge = load_metric(\"rouge\")\n",
        "    for article, summary in zip(dataset[\"article\"], dataset[\"highlights\"]):\n",
        "        generated_summary = generate_summary(article)\n",
        "        scores = rouge.compute(predictions=[generated_summary], references=[summary])\n",
        "        print(f\"Original summary: {summary}\")\n",
        "        print(f\"Generated summary: {generated_summary}\")\n",
        "        print(f\"ROUGE-L: {scores['rouge-l']['l'].item():.4f}\")\n",
        "        print(f\"ROUGE-2: {scores['rouge-2']['f'].item():.4f}\")\n",
        "        print(f\"ROUGE-1: {scores['rouge-1']['f'].item():.4f}\")\n",
        "\n",
        "evaluate_model(model_bart_cnn, cnn_dm_dataset[\"test\"])\n"
      ],
      "metadata": {
        "id": "0q_S9U39bFTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XrwDwaAVbFQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ArnWK76abFOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rGrdatZBbFLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1tlQ8Yz9bFIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cj4FvohxbFFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tCIo7f2VbE_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2TvEFq_sbE9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dSdKP2F_bE6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "foyXPRs_bE3J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}